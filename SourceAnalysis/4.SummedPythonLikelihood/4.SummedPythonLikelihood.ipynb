{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summed Likelihood Analysis with Python\n",
    "\n",
    "This sample analysis shows a way of performing joint likelihood on two data selections using the same XML model. This is useful if you want to do the following:\n",
    "\n",
    "* Coanalysis of Front and Back selections (not using the combined IRF)\n",
    "* Coanalysis of separate time intervals\n",
    "* Coanalysis of separate energy ranges\n",
    "* Pass 8 PSF type analysis\n",
    "* Pass 8 EDISP type analysis\n",
    "\n",
    "This tutorial also assumes that you've gone through the standard [binned likelihood](https://fermi.gsfc.nasa.gov/ssc/data/analysis/scitools/binned_likelihood_tutorial.html) thread using the combined front + back events, to which we will compare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the data\n",
    "\n",
    "For this thread the original data were extracted from the [LAT data server](https://fermi.gsfc.nasa.gov/cgi-bin/ssc/LAT/LATDataQuery.cgi) with the following selections (these selections are similar to those in the paper):\n",
    "\n",
    "```\n",
    "Search Center (RA,Dec) = (193.98,-5.82)\n",
    "Radius = 15 degrees\n",
    "Start Time (MET) = 239557417 seconds (2008-08-04T15:43:37)\n",
    "Stop Time (MET) = 302572802 seconds (2010-08-04T00:00:00)\n",
    "Minimum Energy = 100 MeV\n",
    "Maximum Energy = 500000 MeV\n",
    "```\n",
    "\n",
    "For more information on how to download LAT data please see the [Extract LAT Data](https://fermi.gsfc.nasa.gov/ssc/data/analysis/scitools/extract_latdata.html) tutorial.\n",
    "\n",
    "These are the event files. Run the code cell below to retrieve them:\n",
    "```\n",
    "L181126210218F4F0ED2738_PH00.fits (5.4 MB)\n",
    "L181126210218F4F0ED2738_PH01.fits (10.8 MB)\n",
    "L181126210218F4F0ED2738_PH02.fits (6.9 MB)\n",
    "L181126210218F4F0ED2738_PH03.fits (9.8 MB)\n",
    "L181126210218F4F0ED2738_PH04.fits (7.8 MB)\n",
    "L181126210218F4F0ED2738_PH05.fits (6.6 MB)\n",
    "L181126210218F4F0ED2738_PH06.fits (4.8 MB)\n",
    "L181126210218F4F0ED2738_SC00.fits (256 MB spacecraft file)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://fermi.gsfc.nasa.gov/ssc/data/analysis/scitools/data/BinnedLikelihood/L181126210218F4F0ED2738_PH00.fits\n",
    "!wget https://fermi.gsfc.nasa.gov/ssc/data/analysis/scitools/data/BinnedLikelihood/L181126210218F4F0ED2738_PH01.fits\n",
    "!wget https://fermi.gsfc.nasa.gov/ssc/data/analysis/scitools/data/BinnedLikelihood/L181126210218F4F0ED2738_PH02.fits\n",
    "!wget https://fermi.gsfc.nasa.gov/ssc/data/analysis/scitools/data/BinnedLikelihood/L181126210218F4F0ED2738_PH03.fits\n",
    "!wget https://fermi.gsfc.nasa.gov/ssc/data/analysis/scitools/data/BinnedLikelihood/L181126210218F4F0ED2738_PH04.fits\n",
    "!wget https://fermi.gsfc.nasa.gov/ssc/data/analysis/scitools/data/BinnedLikelihood/L181126210218F4F0ED2738_PH05.fits\n",
    "!wget https://fermi.gsfc.nasa.gov/ssc/data/analysis/scitools/data/BinnedLikelihood/L181126210218F4F0ED2738_PH06.fits\n",
    "!wget https://fermi.gsfc.nasa.gov/ssc/data/analysis/scitools/data/BinnedLikelihood/L181126210218F4F0ED2738_SC00.fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data\n",
    "!mv *.fits ./data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll first need to make a file list with the names of your input event files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ./data/*_PH*.fits > ./data/binned_events.txt\n",
    "!cat ./data/binned_events.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following analysis we've assumed that you've named your list of data files `binned_events.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Event Selections\n",
    "\n",
    "You could follow the unbinned likelihood tutorial to perform your event selections using **gtlike*, **gtmktime**, etc. directly from the command line, and then use pylikelihood later.\n",
    "\n",
    "But we're going to go ahead and use python. The `gt_apps` module provides methods to call these tools from within python. This'll get us used to using python.\n",
    "\n",
    "So, let's jump into python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gt_apps as my_apps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to run **gtselect** (called `filter` in python) twice. Once, we select only the front events and the other time we select only back events. You do this with `evtype=1` (front) and `evtype=2` (back)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_apps.filter['evclass'] = 128\n",
    "my_apps.filter['evtype'] = 1\n",
    "my_apps.filter['ra'] = 193.98\n",
    "my_apps.filter['dec'] = -5.82\n",
    "my_apps.filter['rad'] = 15\n",
    "my_apps.filter['emin'] = 100\n",
    "my_apps.filter['emax'] = 500000\n",
    "my_apps.filter['zmax'] = 90\n",
    "my_apps.filter['tmin'] = 239557417\n",
    "my_apps.filter['tmax'] = 302572802\n",
    "my_apps.filter['infile'] = '@./data/binned_events.txt'\n",
    "my_apps.filter['outfile'] = './data/3C279_front_filtered.fits'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this is done, we can run **gtselect**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_apps.filter.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we select the back events and run it again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_apps.filter['evtype'] = 2\n",
    "my_apps.filter['outfile'] = './data/3C279_back_filtered.fits'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_apps.filter.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to find the GTIs for each data set (front and back). This is accessed within python via the `maketime` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Front\n",
    "my_apps.maketime['scfile'] = './data/L181126210218F4F0ED2738_SC00.fits'\n",
    "my_apps.maketime['filter'] = '(DATA_QUAL>0)&&(LAT_CONFIG==1)'\n",
    "my_apps.maketime['roicut'] = 'no'\n",
    "my_apps.maketime['evfile'] = './data/3C279_front_filtered.fits'\n",
    "my_apps.maketime['outfile'] = './data/3C279_front_filtered_gti.fits'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_apps.maketime.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar for the back:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Back\n",
    "my_apps.maketime['evfile'] = './data/3C279_back_filtered.fits'\n",
    "my_apps.maketime['outfile'] = './data/3C279_back_filtered_gti.fits'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_apps.maketime.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Livetime and Counts Cubes\n",
    "\n",
    "### Livetime Cube\n",
    "\n",
    "We can now compute the livetime cube. We only need to do this once since in this case we made the exact same time cuts and used the same GTI filter on front and back datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_apps.expCube['evfile'] = './data/3C279_front_filtered_gti.fits'\n",
    "my_apps.expCube['scfile'] = './data/L181126210218F4F0ED2738_SC00.fits'\n",
    "my_apps.expCube['outfile'] = './data/3C279_front_ltcube.fits'\n",
    "my_apps.expCube['zmax'] = 90\n",
    "my_apps.expCube['dcostheta'] = 0.025\n",
    "my_apps.expCube['binsz'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_apps.expCube.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counts Cube\n",
    "\n",
    "The counts cube is the counts from our data file binned in space and energy. All of the steps above use a circular ROI (or a cone, really).\n",
    "\n",
    "Once you switch to binned analysis, you start doing things in squares. Your counts cube can only be as big as the biggest square that can fit in the circular ROI you already selected.\n",
    "\n",
    "We start with front events:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_apps.evtbin['evfile'] = './data/3C279_front_filtered_gti.fits'\n",
    "my_apps.evtbin['outfile'] = './data/3C279_front_ccube.fits'\n",
    "my_apps.evtbin['algorithm'] = 'CCUBE'\n",
    "my_apps.evtbin['nxpix'] = 100\n",
    "my_apps.evtbin['nypix'] = 100\n",
    "my_apps.evtbin['binsz'] = 0.2\n",
    "my_apps.evtbin['coordsys'] = 'CEL'\n",
    "my_apps.evtbin['xref'] = 193.98\n",
    "my_apps.evtbin['yref'] = -5.82\n",
    "my_apps.evtbin['axisrot'] = 0\n",
    "my_apps.evtbin['proj'] = 'AIT'\n",
    "my_apps.evtbin['ebinalg'] = 'LOG'\n",
    "my_apps.evtbin['emin'] = 100\n",
    "my_apps.evtbin['emax'] = 500000\n",
    "my_apps.evtbin['enumbins'] = 37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_apps.evtbin.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then for the back events:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_apps.evtbin['evfile'] = './data/3C279_back_filtered_gti.fits'\n",
    "my_apps.evtbin['outfile'] = './data/3C279_back_ccube.fits'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_apps.evtbin.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exposure Maps\n",
    "\n",
    "The binned exposure map is an exposure map binned in space and energy.\n",
    "\n",
    "We first need to import the python version of `gtexpcube2`, which doesn't have a gtapp version by default. This is easy to do (you can import any of the command line tools into python this way). Then, you can check out the parameters with the `pars` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GtApp import GtApp\n",
    "expCube2= GtApp('gtexpcube2','Likelihood')\n",
    "expCube2.pars()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we generate exposure maps for the entire sky. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expCube2['infile'] = './data/3C279_front_ltcube.fits'\n",
    "expCube2['cmap'] = 'none'\n",
    "expCube2['outfile'] = './data/3C279_front_BinnedExpMap.fits'\n",
    "expCube2['irfs'] = 'P8R3_SOURCE_V2'\n",
    "expCube2['evtype'] = '1'\n",
    "expCube2['nxpix'] = 1800\n",
    "expCube2['nypix'] = 900\n",
    "expCube2['binsz'] = 0.2\n",
    "expCube2['coordsys'] = 'CEL'\n",
    "expCube2['xref'] = 193.98\n",
    "expCube2['yref'] = -5.82\n",
    "expCube2['axisrot'] = 0\n",
    "expCube2['proj'] = 'AIT'\n",
    "expCube2['ebinalg'] = 'LOG'\n",
    "expCube2['emin'] = 100\n",
    "expCube2['emax'] = 500000\n",
    "expCube2['enumbins'] = 37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expCube2.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expCube2['infile'] = './data/3C279_front_ltcube.fits'\n",
    "expCube2['outfile'] = './data/3C279_back_BinnedExpMap.fits'\n",
    "expCube2['evtype'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expCube2.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Source Maps\n",
    "\n",
    "The source maps step convolves the LAT response with your source model, generating maps for each source in the model for use in the likelihood calculation.\n",
    "\n",
    "We use the same [XML](https://fermi.gsfc.nasa.gov/ssc/data/analysis/scitools/data/BinnedLikelihood/3C279_input_model.xml) file as in the standard [binned likelihood](https://fermi.gsfc.nasa.gov/ssc/data/analysis/scitools/binned_likelihood_tutorial.html) analysis.\n",
    "\n",
    "If you want to make your own model filey, you should also download the recommended models for a normal point source analysis `gll_iem_v07.fits` and `iso_P8R3_SOURCE_V3_v1.txt`.\n",
    "\n",
    "We'll take a shortcut, however, and simply copy the `3C279_input_model.xml` we made in the Binned Likelihood notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp ../1.BinnedLikelihood/data/3C279_input_model.xml ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv *.xml ./data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the files `gll_iem_v07.fits` and `iso_P8R3_SOURCE_V2_v1.txt` must be in your current working directory for the next steps to work.\n",
    "\n",
    "We compute the front events:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_apps.srcMaps['expcube'] = './data/3C279_front_ltcube.fits'\n",
    "my_apps.srcMaps['cmap'] = './data/3C279_front_ccube.fits'\n",
    "my_apps.srcMaps['srcmdl'] = './data/3C279_input_model.xml'\n",
    "my_apps.srcMaps['bexpmap'] = './data/3C279_front_BinnedExpMap.fits'\n",
    "my_apps.srcMaps['outfile'] = './data/3C279_front_srcmap.fits'\n",
    "my_apps.srcMaps['irfs'] = 'P8R3_SOURCE_V3'\n",
    "my_apps.srcMaps['evtype'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_apps.srcMaps.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And similarly, the back events:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_apps.srcMaps['expcube'] = './data/3C279_front_ltcube.fits'\n",
    "my_apps.srcMaps['cmap'] = './data/3C279_back_ccube.fits'\n",
    "my_apps.srcMaps['srcmdl'] = './data/3C279_input_model.xml'\n",
    "my_apps.srcMaps['bexpmap'] = './data/3C279_back_BinnedExpMap.fits'\n",
    "my_apps.srcMaps['outfile'] = './data/3C279_back_srcmap.fits'\n",
    "my_apps.srcMaps['irfs'] = 'P8R3_SOURCE_V3'\n",
    "my_apps.srcMaps['evtype'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_apps.srcMaps.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the Likelihood Analysis\n",
    "\n",
    "First, import the BinnedAnalysis and SummedAnalysis libraries. Then, create a likelihood object for both the front and the back datasets. For more details on the pyLikelihood module, check out the [pyLikelihood Usage Notes](https://fermi.gsfc.nasa.gov/ssc/data/analysis/scitools/python_usage_notes.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLikelihood\n",
    "from BinnedAnalysis import *\n",
    "from SummedLikelihood import *\n",
    "\n",
    "front = BinnedObs(srcMaps='./data/3C279_front_srcmap.fits',binnedExpMap='./data/3C279_front_BinnedExpMap.fits',expCube='./data/3C279_front_ltcube.fits',irfs='CALDB')\n",
    "likefront = BinnedAnalysis(front,'./data/3C279_input_model.xml',optimizer='NewMinuit')\n",
    "back = BinnedObs(srcMaps='./data/3C279_back_srcmap.fits',binnedExpMap='./data/3C279_back_BinnedExpMap.fits',expCube='./data/3C279_front_ltcube.fits',irfs='CALDB')\n",
    "likeback = BinnedAnalysis(back,'./data/3C279_input_model.xml',optimizer='NewMinuit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, create the summedlikelihood object and add the two likelihood objects, one for the front selection and the second for the back selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summed_like = SummedLikelihood()\n",
    "summed_like.addComponent(likefront)\n",
    "summed_like.addComponent(likeback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the fit and print out the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summedobj = pyLike.NewMinuit(summed_like.logLike)\n",
    "summed_like.fit(verbosity=0,covar=True,optObject=summedobj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print TS for 3C 279 (4FGL J1256.1-0547):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summed_like.Ts('4FGL J1256.1-0547')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compare to the standard [binned likelihood](https://fermi.gsfc.nasa.gov/ssc/data/analysis/scitools/binned_likelihood_tutorial.html) analysis that uses only one data set containing both Front and Back event types that are represented by a single, combined IRF set. You will need to download the files created in that analysis thread or rerun this python tutorial with the combined dataset `(evtype=3)`.\n",
    "\n",
    "For your convenience, the files can be obtained from the code cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://fermi.gsfc.nasa.gov/ssc/data/analysis/scitools/data/BinnedLikelihood/3C279_binned_srcmaps.fits\n",
    "!wget https://fermi.gsfc.nasa.gov/ssc/data/analysis/scitools/data/BinnedLikelihood/3C279_binned_allsky_expcube.fits\n",
    "!wget https://fermi.gsfc.nasa.gov/ssc/data/analysis/scitools/data/BinnedLikelihood/3C279_binned_ltcube.fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv 3C279*.fits ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = BinnedObs(srcMaps='./data/3C279_binned_srcmaps.fits',binnedExpMap='./data/3C279_binned_allsky_expcube.fits',expCube='./data/3C279_binned_ltcube.fits',irfs='CALDB')\n",
    "likeall = BinnedAnalysis(all,'./data/3C279_input_model.xml',optimizer='NewMinuit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the fit and print out the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likeallobj = pyLike.NewMinuit(likeall.logLike)\n",
    "likeall.fit(verbosity=0,covar=True,optObject=likeallobj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print TS for 3C 279 (4FGL J1256.1-0547):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likeall.Ts('4FGL J1256.1-0547')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TS for the front + back analysis is ~29261, a bit lower than what we found for the separate front and back analysis 30191.550.\n",
    "\n",
    "The important difference is that in the separated version of the analysis each event type has a dedicated response function set instead of using the averaged Front+Back response. This should increase the sensitivity, and therefore, the TS value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
